# Adapter-Hub adapter entry
# Defines a single adapter entry in Adapter-Hub
# --------------------

# The type of adapter (one of the options available in `adapter_type`.
type: text_task

# The string identifier of the task this adapter belongs to.
task: mlki

# The string identifier of the subtask this adapter belongs to.
subtask: ep

# The model type.
# Example: bert
model_type: xlm-roberta

# The string identifier of the pre-trained model (by which it is identified at Huggingface).
# Example: bert-base-uncased
model_name: xlm-roberta-large

# The name of the author(s) of this adapter.
author: Yifan Hou

# Describes the adapter architecture used by this adapter
config:
  # The name of the adapter config used by this adapter (a short name available in the `architectures` folder).
  # Example: pfeiffer
  using: pfeiffer
  non_linearity: relu
  reduction_factor: 16
default_version: '1'

# A list of different versions of this adapter available for download.
files:
- version: '1'
  url: https://github.com/yifan-h/Multilingual_Space/tree/main/adapters/xlm-roberta-large_mlki_ep_pfeiffer.zip
  sha1: d80aa30c9b10b4ca0f2049f6b4a76a994c1a298f
  sha256: 528dc2ab87f258febca0d69676b51512c0c7414ff77d96ec5a0ee31b3ce1d64f
citation: '@article{hou2022adapters,
  title={Adapters for Enhanced Modeling of Multilingual Knowledge and Text},
  author={Hou, Yifan and Jiao, Wenxiang and Liu, Meizhen and Allen, Carl and Tu, Zhaopeng and Sachan, Mrinmaya},
  journal={arXiv preprint arXiv:2210.13617},
  year={2022}
}'


# (optional) A short description of this adapter.
description: 'Knowledge adapter set for multilingual knowledge graph integration. This adapter is for cross-lingual entity alignment enhancement (phrase-level). We trained it with alignments from Wikidata across 84 languages.'

# (optional) A contact email of the author(s).
email: yifan.hou@inf.ethz.ch

# (optional) A GitHub handle associated with the author(s).
github: eth-nlped

# (optional) The name of the model class from which this adapter was extracted. This field is mainly intended for adapters with prediction heads.
# Example: BertModelWithHeads
model_class: XLMRobertaModel

# (optional) If the adapter has a pre-trained prediction head included.
prediction_head: false

# (optional) A Twitter handle associated with the author(s).
twitter: https://twitter.com/yyyyyyyyifan

# (optional) A URL providing more information on this adapter/ the authors/ the organization.
url: https://yifan-h.github.io/
